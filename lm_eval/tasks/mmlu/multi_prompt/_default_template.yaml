# `cais/mmlu` with no auxiliary_train split and with 100 prompt templates 
# variations per task
task: mmlu_multi_prompt
dataset_path: PromptEval/MMLU_multi_prompt 
dataset_name: all
task_alias: all
test_split: test
fewshot_split: dev
num_fewshot: 0
fewshot_config:
  sampler: first_n
  doc_to_text: "{{model_prompt}}"
  doc_to_target: ""
output_type: multiple_choice
doc_to_text: "{{template}}"
doc_to_choice: ["A", "B", "C", "D"]
doc_to_target: answer
metric_list:
  - metric: acc
    aggregation: mean
    higher_is_better: true
metadata:
  version: 1.0
dataset_kwargs:
  trust_remote_code: true
